{
    "dataset": {
        "name": "az://snlp-data/explainink/fair_rationales_sst2",
        "args": {},
        "text_column": "text",
        "label_column": "label"
    },
    "model": {
        "class": "TopAttentionHuggingFaceClassifier",
        "checkpoint": "/data/research/users/jgonzalez/Users.jagb.ExplainInk/checkpoints/fair_rationales_sst2/top_attention_mask_special_tokens/bert/checkpoint-70/model.safetensors",
        "params": {
            "encoder_params": {
                "pretrained_model_name_or_path": "bert-base-uncased"
            },
            "tokenizer_params": {},
            "classifier_params": {
                "num_labels": 2,
                "dropout": 0.1
            },
            "attention_params": {
                "class_name": "ContextAttentionLayer",
                "attention_kl_loss": false,
                "add_embeddings": false,
                "mask_special_tokens": true,
                "conicity_loss": true
            },
            "training_params": {},
            "inference_params": {
                "per_device_eval_batch_size": 32,
                "output_dir": "./tmp"
            }
        }
    }
}